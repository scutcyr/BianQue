# [æ‰é¹Šï¼ˆBianQueï¼‰]((https://github.com/scutcyr/BianQue))
<p align="center">
    <img src="./ProactiveHealthGPT.png" width=900px/>
</p>
<p align="center">
    <a href="./LICENSE"><img src="https://img.shields.io/badge/license-Apache%202-red.svg"></a>
    <a href="support os"><img src="https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg"></a>
    <a href=""><img src="https://img.shields.io/badge/python-3.8+-aff.svg"></a>
    <a href="https://github.com/scutcyr/BianQue/graphs/contributors"><img src="https://img.shields.io/github/contributors/scutcyr/BianQue?color=9ea"></a>
    <a href="https://github.com/scutcyr/BianQue/commits"><img src="https://img.shields.io/github/commit-activity/m/scutcyr/BianQue?color=3af"></a>
    <a href="https://github.com/scutcyr/BianQue/issues"><img src="https://img.shields.io/github/issues/scutcyr/BianQue?color=9cc"></a>
    <a href="https://github.com/scutcyr/BianQue/stargazers"><img src="https://img.shields.io/github/stars/scutcyr/BianQue?color=ccf"></a>
</p>

åŸºäºä¸»åŠ¨å¥åº·çš„ä¸»åŠ¨æ€§ã€é¢„é˜²æ€§ã€ç²¾ç¡®æ€§ã€ä¸ªæ€§åŒ–ã€å…±å»ºå…±äº«ã€è‡ªå¾‹æ€§å…­å¤§ç‰¹å¾ï¼Œåå·¥æœªæ¥æŠ€æœ¯å­¦é™¢-å¹¿ä¸œçœæ•°å­—å­ªç”Ÿäººé‡ç‚¹å®éªŒå®¤å¼€æºäº†ä¸­æ–‡é¢†åŸŸç”Ÿæ´»ç©ºé—´ä¸»åŠ¨å¥åº·å¤§æ¨¡å‹åŸºåº§ProactiveHealthGPTï¼ŒåŒ…æ‹¬ï¼š
* ç»è¿‡åƒä¸‡è§„æ¨¡ä¸­æ–‡å¥åº·å¯¹è¯æ•°æ®æŒ‡ä»¤å¾®è°ƒçš„[ç”Ÿæ´»ç©ºé—´å¥åº·å¤§æ¨¡å‹æ‰é¹Šï¼ˆBianQueï¼‰](https://github.com/scutcyr/BianQue)    
* ç»è¿‡ç™¾ä¸‡è§„æ¨¡å¿ƒç†å’¨è¯¢é¢†åŸŸä¸­æ–‡é•¿æ–‡æœ¬æŒ‡ä»¤ä¸å¤šè½®å…±æƒ…å¯¹è¯æ•°æ®è”åˆæŒ‡ä»¤å¾®è°ƒçš„[å¿ƒç†å¥åº·å¤§æ¨¡å‹çµå¿ƒï¼ˆSoulChatï¼‰](https://github.com/scutcyr/SoulChat)   

æˆ‘ä»¬æœŸæœ›ï¼Œ**ç”Ÿæ´»ç©ºé—´ä¸»åŠ¨å¥åº·å¤§æ¨¡å‹åŸºåº§ProactiveHealthGPT** å¯ä»¥å¸®åŠ©å­¦æœ¯ç•ŒåŠ é€Ÿå¤§æ¨¡å‹åœ¨æ…¢æ€§ç—…ã€å¿ƒç†å’¨è¯¢ç­‰ä¸»åŠ¨å¥åº·é¢†åŸŸçš„ç ”ç©¶ä¸åº”ç”¨ã€‚æœ¬é¡¹ç›®ä¸º **ç”Ÿæ´»ç©ºé—´å¥åº·å¤§æ¨¡å‹æ‰é¹Šï¼ˆBianQueï¼‰** ã€‚

## æœ€è¿‘æ›´æ–°
- ğŸ‘ğŸ»  2023.06.06: æ‰é¹Š-2.0æ¨¡å‹å¼€æºï¼Œè¯¦æƒ…è§[BianQue-2.0](https://huggingface.co/scutcyr/BianQue-2.0)ã€‚
- ğŸ‘ğŸ»  2023.06.06: å…·å¤‡å…±æƒ…ä¸å€¾å¬èƒ½åŠ›çš„çµå¿ƒå¥åº·å¤§æ¨¡å‹SoulChatå‘å¸ƒï¼Œè¯¦æƒ…è§ï¼š[çµå¿ƒå¥åº·å¤§æ¨¡å‹SoulChatï¼šé€šè¿‡é•¿æ–‡æœ¬å’¨è¯¢æŒ‡ä»¤ä¸å¤šè½®å…±æƒ…å¯¹è¯æ•°æ®é›†çš„æ··åˆå¾®è°ƒï¼Œæå‡å¤§æ¨¡å‹çš„â€œå…±æƒ…â€èƒ½åŠ› ](https://huggingface.co/scutcyr/SoulChat)ã€‚
- ğŸ‘ğŸ»  2023.04.22: åŸºäºæ‰é¹Š-1.0æ¨¡å‹çš„åŒ»ç–—é—®ç­”ç³»ç»ŸDemoï¼Œè¯¦æƒ…è®¿é—®ï¼š[https://huggingface.co/spaces/scutcyr/BianQue](https://huggingface.co/spaces/scutcyr/BianQue)
- ğŸ‘ğŸ»  2023.04.22: æ‰é¹Š-1.0ç‰ˆæœ¬æ¨¡å‹å‘å¸ƒï¼Œè¯¦æƒ…è§ï¼š[æ‰é¹Š-1.0ï¼šé€šè¿‡æ··åˆæŒ‡ä»¤å’Œå¤šè½®åŒ»ç”Ÿé—®è¯¢æ•°æ®é›†çš„å¾®è°ƒï¼Œæé«˜åŒ»ç–—èŠå¤©æ¨¡å‹çš„â€œé—®â€èƒ½åŠ›ï¼ˆBianQue-1.0: Improving the "Question" Ability of Medical Chat Model through finetuning with Hybrid Instructions and Multi-turn Doctor QA Datasetsï¼‰](https://huggingface.co/scutcyr/BianQue-1.0)


### æ‰é¹Šå¥åº·å¤§æ•°æ®BianQueCorpus
æˆ‘ä»¬ç»è¿‡è°ƒç ”å‘ç°ï¼Œåœ¨å¥åº·é¢†åŸŸï¼Œç”¨æˆ·é€šå¸¸ä¸ä¼šåœ¨ä¸€è½®äº¤äº’å½“ä¸­æ¸…æ™°åœ°æè¿°è‡ªå·±çš„é—®é¢˜ï¼Œè€Œå½“å‰å¸¸è§çš„å¼€æºåŒ»ç–—é—®ç­”æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼šChatDoctorã€æœ¬è‰(HuaTuoï¼ŒåŸååé©¼ )ã€DoctorGLMã€MedicalGPT-zhï¼‰ä¾§é‡äºè§£å†³å•è½®ç”¨æˆ·æè¿°çš„é—®é¢˜ï¼Œè€Œå¿½ç•¥äº†â€œç”¨æˆ·æè¿°å¯èƒ½å­˜åœ¨ä¸è¶³â€çš„æƒ…å†µã€‚å“ªæ€•æ˜¯å½“å‰å¤§ç«çš„ChatGPTä¹Ÿä¼šå­˜åœ¨ç±»ä¼¼çš„é—®é¢˜ï¼šå¦‚æœç”¨æˆ·ä¸å¼ºåˆ¶é€šè¿‡æ–‡æœ¬æè¿°è®©ChatGPTé‡‡ç”¨ä¸€é—®ä¸€ç­”çš„å½¢å¼ï¼ŒChatGPTä¹Ÿåå‘äºé’ˆå¯¹ç”¨æˆ·çš„æè¿°ï¼Œè¿…é€Ÿç»™å‡ºå®ƒè®¤ä¸ºåˆé€‚çš„å»ºè®®å’Œæ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå®é™…çš„åŒ»ç”Ÿä¸ç”¨æˆ·äº¤è°ˆå¾€å¾€ä¼šå­˜åœ¨â€œåŒ»ç”Ÿæ ¹æ®ç”¨æˆ·å½“å‰çš„æè¿°è¿›è¡ŒæŒç»­å¤šè½®çš„è¯¢é—®â€ã€‚å¹¶ä¸”åŒ»ç”Ÿåœ¨æœ€åæ ¹æ®ç”¨æˆ·æä¾›çš„ä¿¡æ¯ç»¼åˆç»™å‡ºå»ºè®®ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚æˆ‘ä»¬æŠŠåŒ»ç”Ÿä¸æ–­é—®è¯¢çš„è¿‡ç¨‹å®šä¹‰ä¸º **è¯¢é—®é“¾ï¼ˆCoQ, Chain of Questioningï¼‰** ï¼Œå½“æ¨¡å‹å¤„äºè¯¢é—®é“¾é˜¶æ®µï¼Œå…¶ä¸‹ä¸€ä¸ªé—®é¢˜é€šå¸¸ç”±å¯¹è¯ä¸Šä¸‹æ–‡å†å²å†³å®šã€‚

<p align="center">
    <img src="./figure/coq.png" width=900px/>
</p>


æˆ‘ä»¬ç»“åˆå½“å‰å¼€æºçš„ä¸­æ–‡åŒ»ç–—é—®ç­”æ•°æ®é›†ï¼ˆ[MedDialog-CN](https://github.com/UCSD-AI4H/Medical-Dialogue-System)ã€[IMCS-V2](https://github.com/lemuria-wchen/imcs21)ã€[CHIP-MDCFNPC](https://tianchi.aliyun.com/dataset/95414)ã€[MedDG](https://tianchi.aliyun.com/dataset/95414)ã€[cMedQA2](https://github.com/zhangsheng93/cMedQA2)ã€[Chinese-medical-dialogue-data](https://github.com/Toyhom/Chinese-medical-dialogue-data)ï¼‰ï¼Œï¼Œåˆ†æå…¶ä¸­çš„å•è½®/å¤šè½®ç‰¹æ€§ä»¥åŠåŒ»ç”Ÿé—®è¯¢ç‰¹æ€§ï¼Œç»“åˆå®éªŒå®¤é•¿æœŸè‡ªå»ºçš„ç”Ÿæ´»ç©ºé—´å¥åº·å¯¹è¯å¤§æ•°æ®ï¼Œæ„å»ºäº†åƒä¸‡çº§åˆ«è§„æ¨¡çš„æ‰é¹Šå¥åº·å¤§æ•°æ®BianQueCorpusã€‚å¯¹è¯æ•°æ®é€šè¿‡â€œç—…äººï¼šxxx\nåŒ»ç”Ÿï¼šxxx\nç—…äººï¼šxxx\nåŒ»ç”Ÿï¼šâ€çš„å½¢å¼ç»Ÿä¸€ä¸ºä¸€ç§æŒ‡ä»¤æ ¼å¼ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

<p align="center">
    <img src="./figure/dataset_example.png" width=900px/>
</p>


```data
input: "ç—…äººï¼šå…­å²å®å®æ‹‰å¤§ä¾¿éƒ½æ˜¯ä¸€ä¸ªç¤¼æ‹œæˆ–è€…10å¤©æ‰ä¸€æ¬¡æ­£å¸¸å—ï¼Œè¦å»åŒ»é™¢æ£€æŸ¥ä»€ä¹ˆé¡¹ç›®\nåŒ»ç”Ÿï¼šæ‚¨å¥½\nç—…äººï¼šå…­å²å®å®æ‹‰å¤§ä¾¿éƒ½æ˜¯ä¸€ä¸ªç¤¼æ‹œæˆ–è€…10å¤©æ‰ä¸€æ¬¡æ­£å¸¸å—ï¼Œè¦å»åŒ»é™¢æ£€æŸ¥ä»€ä¹ˆé¡¹ç›®\nåŒ»ç”Ÿï¼šå®å®ä¹‹å‰å¤§ä¾¿ä»€ä¹ˆæ ·å‘¢ï¼Ÿå¤šä¹…ä¸€æ¬¡å‘¢\nç—…äººï¼šä¸€èˆ¬éƒ½æ˜¯ä¸€ä¸ªç¤¼æ‹œï¼Œæœ€è¿‘è¿™å‡ ä¸ªæœˆéƒ½æ˜¯10å¤šå¤©\nåŒ»ç”Ÿï¼šå¤§ä¾¿å¹²å—ï¼Ÿ\nç—…äººï¼šæ¯æ¬¡10å¤šå¤©æ‹‰çš„å¾ˆå¤š\nåŒ»ç”Ÿï¼š"
target: "æˆå½¢è¿˜æ˜¯ä¸æˆå½¢å‘¢ï¼Ÿå­©å­åƒé¥­æ€ä¹ˆæ ·å‘¢ï¼Ÿ"
```

è®­ç»ƒæ•°æ®å½“ä¸­æ··åˆäº†å¤§é‡targetæ–‡æœ¬ä¸º**åŒ»ç”Ÿé—®è¯¢çš„å†…å®¹**è€Œéç›´æ¥çš„å»ºè®®ï¼Œè¿™å°†æœ‰åŠ©äºæå‡AIæ¨¡å‹çš„é—®è¯¢èƒ½åŠ›ã€‚


## ä½¿ç”¨æ–¹æ³•
* å…‹éš†æœ¬é¡¹ç›®
```bash
cd ~
git clone https://github.com/scutcyr/BianQue.git
```

* å®‰è£…ä¾èµ–
éœ€è¦æ³¨æ„çš„æ˜¯torchçš„ç‰ˆæœ¬éœ€è¦æ ¹æ®ä½ çš„æœåŠ¡å™¨å®é™…çš„cudaç‰ˆæœ¬é€‰æ‹©ï¼Œè¯¦æƒ…å‚è€ƒ[pytorchå®‰è£…æŒ‡å—](https://pytorch.org/get-started/previous-versions/)
```bash
cd BianQue
conda env create -n proactivehealthgpt_py38 --file proactivehealthgpt_py38.yml
conda activate proactivehealthgpt_py38

pip install cpm_kernels
pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116
```

* åœ¨Pythonå½“ä¸­è°ƒç”¨BianQue-2.0æ¨¡å‹ï¼š
```python
import torch
from transformers import AutoModel, AutoTokenizer
# GPUè®¾ç½®
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# åŠ è½½æ¨¡å‹ä¸tokenizer
model_name_or_path = 'scutcyr/BianQue-2.0'
model = AutoModel.from_pretrained(model_name_or_path, trust_remote_code=True).half()
model.to(device)
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)

# å•è½®å¯¹è¯è°ƒç”¨æ¨¡å‹çš„chatå‡½æ•°
user_input = "æˆ‘çš„å®å®å‘çƒ§äº†ï¼Œæ€ä¹ˆåŠï¼Ÿ"
input_text = "ç—…äººï¼š" + user_input + "\nåŒ»ç”Ÿï¼š"
response, history = model.chat(tokenizer, query=input_text, history=None, max_length=2048, num_beams=1, do_sample=True, top_p=0.75, temperature=0.95, logits_processor=None)

# å¤šè½®å¯¹è¯è°ƒç”¨æ¨¡å‹çš„chatå‡½æ•°
# æ³¨æ„ï¼šæœ¬é¡¹ç›®ä½¿ç”¨"\nç—…äººï¼š"å’Œ"\nåŒ»ç”Ÿï¼š"åˆ’åˆ†ä¸åŒè½®æ¬¡çš„å¯¹è¯å†å²
# æ³¨æ„ï¼šuser_historyæ¯”bot_historyçš„é•¿åº¦å¤š1
user_history = ['ä½ å¥½', 'æˆ‘æœ€è¿‘å¤±çœ äº†']
bot_history = ['æˆ‘æ˜¯åˆ©ç”¨äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œç»“åˆå¤§æ•°æ®è®­ç»ƒå¾—åˆ°çš„æ™ºèƒ½åŒ»ç–—é—®ç­”æ¨¡å‹æ‰é¹Šï¼Œä½ å¯ä»¥å‘æˆ‘æé—®ã€‚']
# æ‹¼æ¥å¯¹è¯å†å²
context = "\n".join([f"ç—…äººï¼š{user_history[i]}\nåŒ»ç”Ÿï¼š{bot_history[i]}" for i in range(len(bot_history))])
input_text = context + "\nç—…äººï¼š" + user_history[-1] + "\nåŒ»ç”Ÿï¼š"

response, history = model.chat(tokenizer, query=input_text, history=None, max_length=2048, num_beams=1, do_sample=True, top_p=0.75, temperature=0.95, logits_processor=None)
```


## æ‰é¹Š-2.0
åŸºäºæ‰é¹Šå¥åº·å¤§æ•°æ®BianQueCorpusï¼Œæˆ‘ä»¬é€‰æ‹©äº† [ChatGLM-6B](https://huggingface.co/THUDM/chatglm-6b) ä½œä¸ºåˆå§‹åŒ–æ¨¡å‹ï¼Œç»è¿‡å…¨é‡å‚æ•°çš„æŒ‡ä»¤å¾®è°ƒè®­ç»ƒå¾—åˆ°äº†[æ–°ä¸€ä»£BianQueã€BianQue-2.0ã€‘](https://huggingface.co/scutcyr/BianQue-2.0)ã€‚ä¸æ‰é¹Š-1.0æ¨¡å‹ä¸åŒçš„æ˜¯ï¼Œæ‰é¹Š-2.0æ‰©å……äº†è¯å“è¯´æ˜ä¹¦æŒ‡ä»¤ã€åŒ»å­¦ç™¾ç§‘çŸ¥è¯†æŒ‡ä»¤ä»¥åŠChatGPTè’¸é¦æŒ‡ä»¤ç­‰æ•°æ®ï¼Œå¼ºåŒ–äº†æ¨¡å‹çš„å»ºè®®ä¸çŸ¥è¯†æŸ¥è¯¢èƒ½åŠ›ã€‚ä»¥ä¸‹ä¸ºä¸¤ä¸ªæµ‹è¯•æ ·ä¾‹ã€‚


* æ ·ä¾‹1ï¼šå®å®ç‰¹åˆ«å–œæ¬¢æ‰“å—ï¼Œæ˜¯ä»€ä¹ˆåŸå› å•Šï¼Œè¯¥æ€ä¹ˆé¢„é˜²å•Š
<p align="center">
    <img src="./figure/example_test1.png" width=600px/>
</p>

* æ ·ä¾‹2ï¼šæˆ‘å¤–å©†è¿‘æ¥èº«ä½“è¶Šæ¥è¶Šå·®äº†ï¼Œå¸¦å¥¹å»åŒ»é™¢æ£€æŸ¥ï¼ŒåŒ»ç”Ÿè¯´å¥¹å¾—äº†è‚¾é™è„‰è¡€æ “ï¼Œæˆ‘ä»¬å…¨å®¶éƒ½å¾ˆæ‹…å¿ƒï¼ŒåŒ»ç”Ÿå¼€äº†å¾ˆå¤šæ³¨å°„ç”¨ä½åˆ†å­é‡è‚ç´ é’™ï¼Œæˆ‘æƒ³é—®å®ƒçš„è¯ç†æ¯’ç†ï¼Ÿ
<p align="center">
    <img src="./figure/example_test2.png" width=600px/>
</p>


## æ‰é¹Š-2.0ä¸æ‰é¹Š-1.0è”åˆä½¿ç”¨ï¼Œå…¼é¡¾å¤šè½®é—®è¯¢ä¸å‡ºè‰²çš„å¥åº·å»ºè®®èƒ½åŠ›
é€šè¿‡ä»¥ä¸‹å‘½ä»¤å®ç°è”åˆä½¿ç”¨æ‰é¹Š-2.0ä¸æ‰é¹Š-1.0æ„å»ºä¸»åŠ¨å¥åº·æœåŠ¡ï¼š
```bash
streamlit run bianque_v1_v2_app.py --server.port 9005
```

ä»¥ä¸‹ä¸ºåº”ç”¨ä¾‹å­ï¼šå‰é¢è‹¥å¹²è½®ä¸ºç»è¿‡æ‰é¹Š-1.0æ¨¡å‹è¿›è¡Œé—®è¯¢çš„è¿‡ç¨‹ï¼Œæœ€åä¸€è½®å›å¤ä¸ºç»è¿‡æ‰é¹Š-2.0æ¨¡å‹çš„å›ç­”ã€‚
<p align="center">
    <img src="./figure/example_multi_turn.png" width=600px/>
</p>


## æ‰é¹Š-1.0

**æ‰é¹Š-1.0ï¼ˆBianQue-1.0ï¼‰** æ˜¯ä¸€ä¸ªç»è¿‡æŒ‡ä»¤ä¸å¤šè½®é—®è¯¢å¯¹è¯è”åˆå¾®è°ƒçš„åŒ»ç–—å¯¹è¯å¤§æ¨¡å‹ã€‚æˆ‘ä»¬ç»è¿‡è°ƒç ”å‘ç°ï¼Œåœ¨åŒ»ç–—é¢†åŸŸï¼Œå¾€å¾€åŒ»ç”Ÿéœ€è¦é€šè¿‡å¤šè½®é—®è¯¢æ‰èƒ½è¿›è¡Œå†³ç­–ï¼Œè¿™å¹¶ä¸æ˜¯å•çº¯çš„â€œæŒ‡ä»¤-å›å¤â€æ¨¡å¼ã€‚ç”¨æˆ·åœ¨å’¨è¯¢åŒ»ç”Ÿæ—¶ï¼Œå¾€å¾€ä¸ä¼šåœ¨æœ€åˆå°±æŠŠå®Œæ•´çš„æƒ…å†µå‘ŠçŸ¥åŒ»ç”Ÿï¼Œå› æ­¤åŒ»ç”Ÿéœ€è¦ä¸æ–­è¿›è¡Œè¯¢é—®ï¼Œæœ€åæ‰èƒ½è¿›è¡Œè¯Šæ–­å¹¶ç»™å‡ºåˆç†çš„å»ºè®®ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº† **æ‰é¹Š-1.0ï¼ˆBianQue-1.0ï¼‰** ï¼Œæ‹Ÿåœ¨ **å¼ºåŒ–AIç³»ç»Ÿçš„é—®è¯¢èƒ½åŠ›** ï¼Œä»è€Œè¾¾åˆ°æ¨¡æ‹ŸåŒ»ç”Ÿé—®è¯Šçš„è¿‡ç¨‹ã€‚æˆ‘ä»¬æŠŠè¿™ç§èƒ½åŠ›å®šä¹‰ä¸ºâ€œæœ›é—»é—®åˆ‡â€å½“ä¸­çš„â€œé—®â€ã€‚ç»¼åˆè€ƒè™‘å½“å‰ä¸­æ–‡è¯­è¨€æ¨¡å‹æ¶æ„ã€å‚æ•°é‡ä»¥åŠæ‰€éœ€è¦çš„ç®—åŠ›ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†[ClueAI/ChatYuan-large-v2](https://huggingface.co/ClueAI/ChatYuan-large-v2)ä½œä¸ºåŸºå‡†æ¨¡å‹ï¼Œåœ¨8å¼  NVIDIA RTX 4090æ˜¾å¡ä¸Šå¾®è°ƒäº†1ä¸ªepochå¾—åˆ°**æ‰é¹Š-1.0ï¼ˆBianQue-1.0ï¼‰**ï¼Œç”¨äºè®­ç»ƒçš„**ä¸­æ–‡åŒ»ç–—é—®ç­”æŒ‡ä»¤ä¸å¤šè½®é—®è¯¢å¯¹è¯æ··åˆæ•°æ®é›†**åŒ…å«äº†è¶…è¿‡900ä¸‡æ¡æ ·æœ¬ï¼Œè¿™èŠ±è´¹äº†å¤§çº¦16å¤©çš„æ—¶é—´å®Œæˆä¸€ä¸ªepochçš„è®­ç»ƒã€‚æˆ‘ä»¬å°†è®¡åˆ’å›´ç»•æ‰é¹Šæ¨¡å‹çš„â€œæœ›é—»é—®åˆ‡â€èƒ½åŠ›ï¼Œç»“åˆåŒ»å­¦ä¸“å®¶çŸ¥è¯†ã€å¤šæ¨¡æ€æŠ€æœ¯ã€å¤šç”Ÿç†ä¿¡å·è®¡ç®—ç­‰ï¼Œè¿›è¡Œå¤šä¸ªç‰ˆæœ¬çš„æ¨¡å‹è¿­ä»£ç ”ç©¶ã€‚æ‰é¹Šï¼ˆBianQueï¼‰æ¨¡å‹æ¬¢è¿ä½ çš„è´¡çŒ®ï¼æˆ‘ä»¬é¼“åŠ±ä½ åœ¨ [BianQue GitHub](https://github.com/scutcyr/BianQue) é¡µé¢æŠ¥å‘Šé—®é¢˜ã€è´¡çŒ® PR å¹¶å‚ä¸è®¨è®ºã€‚æˆ‘ä»¬æœŸå¾…ä¸æ›´å¤šçš„é«˜æ ¡ã€åŒ»é™¢ã€ç ”ç©¶å®éªŒå®¤ã€å…¬å¸ç­‰è¿›è¡Œåˆä½œï¼Œå…±åŒå¼€å±•ä¸‹ä¸€ä»£æ‰é¹Šæ¨¡å‹ç ”ç©¶ã€‚å¯¹äºæ­¤ç±»éœ€æ±‚ï¼ˆä»¥åŠå…¶ä»–ä¸é€‚åˆåœ¨ GitHub ä¸Šæå‡ºçš„éœ€æ±‚ï¼‰ï¼Œè¯·ç›´æ¥å‘é€ç”µå­é‚®ä»¶è‡³ [eeyirongchen@mail.scut.edu.cn](mailto:eeyirongchen@mail.scut.edu.cn)ã€‚


### æ¨¡å‹â€œé—®â€èƒ½åŠ›ç¤ºä¾‹
â€œæœ›é—»é—®åˆ‡â€å››è¯Šæ³•ç”±æ‰é¹Šå‘æ˜ã€‚â€œå››è¯Šæ³•â€ï¼ˆæœ›ã€é—»ã€é—®ã€åˆ‡ï¼‰ï¼Œæ˜¯ä¸­åŒ»è¯Šç—…çš„åŸºæœ¬æ–¹æ³•ã€‚å…¶ä¸­ï¼Œâ€œé—®â€çš„èƒ½åŠ›å¯ä»¥è¢«å»ºæ¨¡åˆ°è¯­è¨€æ¨¡å‹å½“ä¸­ã€‚æˆ‘ä»¬æŠŠAIæ¨¡å‹çš„â€œé—®â€èƒ½åŠ›å®šä¹‰ä¸ºï¼š    
**â€œé—®â€èƒ½åŠ›**ï¼šAIæ¨¡å‹èƒ½å¤Ÿæ ¹æ®ç—…äººè¾“å…¥çš„ä¿¡æ¯ï¼Œç»“åˆåŒ»å­¦çŸ¥è¯†æ¥è¿›è¡Œ**æ°å½“çš„é—®è¯¢**ï¼Œå¹¶ä¸”ç»“åˆè¿›ä¸€æ­¥çš„é—®è¯¢å¾—åˆ°çš„ç—…äººçš„ä¿¡æ¯ï¼Œç»™å‡ºâ€œåˆé€‚çš„è¯Šæ–­æˆ–å»ºè®®â€ã€‚â€œé—®â€çš„è¿‡ç¨‹é€šå¸¸æ˜¯ä¸€ä¸ªå¤šè½®å¯¹è¯è¿‡ç¨‹ã€‚

ä»¥ä¸‹æ˜¯2ä¸ªç®€å•çš„ä¾‹å­ï¼š

* **ä¾‹å­1**    
ç”¨æˆ·ï¼šæˆ‘æœ€è¿‘æ„Ÿè§‰å…¨èº«ç–²æƒ«   
æ‰é¹Š-1.0ï¼šæ˜¯åŠ³ç´¯äº†ï¼Œè¿˜æ˜¯ç†¬å¤œäº†ï¼Ÿ    
ç”¨æˆ·ï¼šè¿™å‘¨éƒ½åœ¨ç†¬å¤œèµ¶è®ºæ–‡    
æ‰é¹Š-1.0ï¼šå—¯ï¼Œç†¬å¤œå®¹æ˜“å¼•èµ·å„ç§ä¸é€‚ï¼Œå¦‚å¤´æ™•ã€ä¹åŠ›ã€å¤±çœ ã€å¤šæ¢¦ç­‰ï¼Œå¤šä¼‘æ¯ï¼Œå¯ä»¥å£æœè°·ç»´ç´ ã€ç»´ç”Ÿç´ B1å’Œå®‰ç¥è¡¥è„‘æ¶²ï¼Œé¿å…ç†¬å¤œã€‚    

* **ä¾‹å­2**   
ç”¨æˆ·ï¼šæˆ‘æœ€è¿‘ç»å¸¸å¤±çœ     
æ‰é¹Š-1.0ï¼šå¤±çœ å¤šä¹…äº†ï¼Ÿ    
ç”¨æˆ·ï¼šä¸¤å‘¨äº†    
æ‰é¹Š-1.0ï¼šç¡çœ æ€ä¹ˆæ ·ï¼Ÿ    
ç”¨æˆ·ï¼šä¸ŠåºŠå‡ å°æ—¶æ‰ç¡å¾—ç€    
æ‰é¹Š-1.0ï¼šå¯ä»¥å£æœä¸€äº›è¯ç‰©   
ç”¨æˆ·ï¼šæœ‰å…·ä½“æ¨èå—   
æ‰é¹Š-1.0ï¼šéœ€è¦å…ˆæ£€æŸ¥ä¸€ä¸‹   
ç”¨æˆ·ï¼šé‚£æˆ‘éœ€è¦åšä»€ä¹ˆæ£€æŸ¥ï¼Ÿ   
æ‰é¹Š-1.0ï¼šå¿ƒç”µå›¾   


### ä½¿ç”¨æ–¹æ³•

#### ç›´æ¥ä½¿ç”¨æ‰é¹Š-1.0æ¨¡å‹

```python
import os
from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoTokenizer
import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


tokenizer = T5Tokenizer.from_pretrained("scutcyr/BianQue-1.0")
model = T5ForConditionalGeneration.from_pretrained("scutcyr/BianQue-1.0")


def preprocess(text):
    text = text.replace("\n", "\\n").replace("\t", "\\t")
    return text

def postprocess(text):
    return text.replace("\\n", "\n").replace("\\t", "\t")

def answer(user_history, bot_history, sample=True, top_p=1, temperature=0.7):
    '''sampleï¼šæ˜¯å¦æŠ½æ ·ã€‚ç”Ÿæˆä»»åŠ¡ï¼Œå¯ä»¥è®¾ç½®ä¸ºTrue;
    top_pï¼š0-1ä¹‹é—´ï¼Œç”Ÿæˆçš„å†…å®¹è¶Šå¤šæ ·
    max_new_tokens=512 lost...'''

    if len(bot_history)>0:
        context = "\n".join([f"ç—…äººï¼š{user_history[i]}\nåŒ»ç”Ÿï¼š{bot_history[i]}" for i in range(len(bot_history))])
        input_text = context + "\nç—…äººï¼š" + user_history[-1] + "\nåŒ»ç”Ÿï¼š"
    else:
        input_text = "ç—…äººï¼š" + user_history[-1] + "\nåŒ»ç”Ÿï¼š"
        return "æˆ‘æ˜¯åˆ©ç”¨äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œç»“åˆå¤§æ•°æ®è®­ç»ƒå¾—åˆ°çš„æ™ºèƒ½åŒ»ç–—é—®ç­”æ¨¡å‹æ‰é¹Šï¼Œä½ å¯ä»¥å‘æˆ‘æé—®ã€‚"
    

    input_text = preprocess(input_text)
    print(input_text)
    encoding = tokenizer(text=input_text, truncation=True, padding=True, max_length=768, return_tensors="pt").to(device) 
    if not sample:
        out = model.generate(**encoding, return_dict_in_generate=True, output_scores=False, max_new_tokens=512, num_beams=1, length_penalty=0.6)
    else:
        out = model.generate(**encoding, return_dict_in_generate=True, output_scores=False, max_new_tokens=512, do_sample=True, top_p=top_p, temperature=temperature, no_repeat_ngram_size=3)
    out_text = tokenizer.batch_decode(out["sequences"], skip_special_tokens=True)
    print('åŒ»ç”Ÿ: '+postprocess(out_text[0]))
    return postprocess(out_text[0])

answer_text = answer(user_history=["ä½ å¥½ï¼",
                                   "æˆ‘æœ€è¿‘ç»å¸¸å¤±çœ ",
                                   "ä¸¤å‘¨äº†",
                                   "ä¸ŠåºŠå‡ å°æ—¶æ‰ç¡å¾—ç€"], 
                     bot_history=["æˆ‘æ˜¯åˆ©ç”¨äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œç»“åˆå¤§æ•°æ®è®­ç»ƒå¾—åˆ°çš„æ™ºèƒ½åŒ»ç–—é—®ç­”æ¨¡å‹æ‰é¹Šï¼Œä½ å¯ä»¥å‘æˆ‘æé—®ã€‚",
                                  "å¤±çœ å¤šä¹…äº†ï¼Ÿ",
                                  "ç¡çœ æ€ä¹ˆæ ·ï¼Ÿ"])
```

#### ä½¿ç”¨ä¸ªäººæ•°æ®åœ¨æ‰é¹Š-1.0æ¨¡å‹åŸºç¡€ä¸Šè¿›ä¸€æ­¥å¾®è°ƒæ¨¡å‹
* ç¯å¢ƒåˆ›å»º   
ä»¥ä¸‹ä¸ºåœ¨RTX 4090æ˜¾å¡ï¼ŒCUDA-11.6é©±åŠ¨é…ç½®ä¸‹çš„ç¯å¢ƒé…ç½®
```bash
conda env create -n bianque_py38 --file py38_conda_env.yml
conda activate bianque_py38
pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116
```
* æ•°æ®é›†æ„å»º   
å‚è€ƒ[.data/cMedialog_example.csv](.data/cMedialog_example.csv)æ ¼å¼ï¼Œæ„å»ºä½ çš„æ•°æ®é›†
* åŸºäºæ‰é¹Š-1.0æ¨¡å‹å¾®è°ƒä½ çš„æ¨¡å‹    
ä¿®æ”¹[./scripts/run_train_model_bianque.sh](./scripts/run_train_model_bianque.sh)ï¼Œé€šè¿‡ç»å¯¹è·¯å¾„æŒ‡å®šPREPROCESS_DATAï¼Œå¹¶ä¸”è°ƒæ•´å…¶ä»–å˜é‡ï¼Œç„¶åè¿è¡Œï¼š
```bash
cd scripts
bash run_train_model_bianque.sh
```


## å£°æ˜

**æ‰é¹Š-1.0ï¼ˆBianQue-1.0ï¼‰** å½“å‰ä»…ç»è¿‡1ä¸ªepochçš„è®­ç»ƒï¼Œå°½ç®¡æ¨¡å‹å…·å¤‡äº†ä¸€å®šçš„åŒ»ç–—é—®è¯¢èƒ½åŠ›ï¼Œä½†å…¶ä»ç„¶å­˜åœ¨ä»¥ä¸‹å±€é™ï¼š
* è®­ç»ƒæ•°æ®æ¥æºäºå¼€æºæ•°æ®é›†ä»¥åŠäº’è”ç½‘ï¼Œå°½ç®¡æˆ‘ä»¬é‡‡ç”¨äº†ä¸¥æ ¼çš„æ•°æ®æ¸…æ´—æµç¨‹ï¼Œæ•°æ®é›†å½“ä¸­ä»ç„¶ä¸å¯é¿å…åœ°å­˜åœ¨å¤§é‡å™ªå£°ï¼Œè¿™ä¼šä½¿å¾—éƒ¨åˆ†å›å¤äº§ç”Ÿé”™è¯¯ï¼›
* åŒ»ç”Ÿâ€œé—®è¯¢â€æ˜¯ä¸€é¡¹å¤æ‚çš„èƒ½åŠ›ï¼Œè¿™æ˜¯éåŒ»ç”Ÿç¾¤ä½“æ‰€ä¸å…·å¤‡çš„ï¼Œå½“å‰çš„æ¨¡å‹å¯¹äºæ¨¡æ‹Ÿâ€œåŒ»ç”Ÿé—®è¯¢â€è¿‡ç¨‹æ˜¯é€šè¿‡å¤§é‡æ ·æœ¬å­¦ä¹ å¾—åˆ°çš„ï¼Œå› æ­¤åœ¨é—®è¯¢è¿‡ç¨‹å½“ä¸­ï¼Œæœ‰å¯èƒ½å‡ºç°ä¸€äº›å¥‡å¼‚çš„æé—®é£æ ¼ã€‚æ¢ä¸€å¥è¯æ¥è¯´ï¼Œå½“å‰ç‰ˆæœ¬çš„æ¨¡å‹å¼ºåŒ–äº†â€œé—®â€çš„èƒ½åŠ›ï¼Œä½†æ˜¯â€œæœ›â€ã€â€œé—»â€ã€â€œåˆ‡â€çš„èƒ½åŠ›ä»å¾…è¿›ä¸€æ­¥ç ”ç©¶ï¼


## å¼•ç”¨
```bib
@article{chen2023bianque1,
      title={BianQue-1.0: Improving the "Question" Ability of Medical Chat Model through finetuning with Hybrid Instructions and Multi-turn Doctor QA Datasets}, 
      author={Yirong Chen and Zhenyu Wang and Xiaofen Xing and Zhipei Xu and Kai Fang and Sihang Li and Junhong Wang and Xiangmin Xu},
      year={2023},
      url={https://github.com/scutcyr/BianQue}
}
```
