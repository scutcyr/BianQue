# [æ‰é¹Šï¼ˆBianQueï¼‰]((https://github.com/scutcyr/BianQie))

<p align="center">
    <a href="./LICENSE"><img src="https://img.shields.io/badge/license-Apache%202-red.svg"></a>
    <a href="support os"><img src="https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg"></a>
    <a href=""><img src="https://img.shields.io/badge/python-3.7+-aff.svg"></a>
    <a href="https://github.com/scutcyr/BianQie/graphs/contributors"><img src="https://img.shields.io/github/contributors/scutcyr/BianQie?color=9ea"></a>
    <a href="https://github.com/scutcyr/BianQie/commits"><img src="https://img.shields.io/github/commit-activity/m/scutcyr/BianQie?color=3af"></a>
    <a href="https://github.com/scutcyr/BianQie/issues"><img src="https://img.shields.io/github/issues/scutcyr/BianQie?color=9cc"></a>
    <a href="https://github.com/scutcyr/BianQie/stargazers"><img src="https://img.shields.io/github/stars/scutcyr/BianQie?color=ccf"></a>
</p>


## æœ€è¿‘æ›´æ–°
- ğŸ‘ğŸ»  2023.04.22: åŸºäºæ‰é¹Š-1.0æ¨¡å‹çš„åŒ»ç–—é—®ç­”ç³»ç»ŸDemoï¼Œè¯¦æƒ…è®¿é—®ï¼š[https://huggingface.co/spaces/scutcyr/BianQue](https://huggingface.co/spaces/scutcyr/BianQue)
- ğŸ‘ğŸ»  2023.04.22: æ‰é¹Š-1.0ç‰ˆæœ¬æ¨¡å‹å‘å¸ƒï¼Œè¯¦æƒ…è§ï¼š[æ‰é¹Š-1.0ï¼šé€šè¿‡æ··åˆæŒ‡ä»¤å’Œå¤šè½®åŒ»ç”Ÿé—®è¯¢æ•°æ®é›†çš„å¾®è°ƒï¼Œæé«˜åŒ»ç–—èŠå¤©æ¨¡å‹çš„â€œé—®â€èƒ½åŠ›ï¼ˆBianQue-1.0: Improving the "Question" Ability of Medical Chat Model through finetuning with Hybrid Instructions and Multi-turn Doctor QA Datasetsï¼‰](https://huggingface.co/scutcyr/BianQue-1.0)


## ç®€ä»‹

**æ‰é¹Š-1.0ï¼ˆBianQue-1.0ï¼‰** æ˜¯ä¸€ä¸ªç»è¿‡æŒ‡ä»¤ä¸å¤šè½®é—®è¯¢å¯¹è¯è”åˆå¾®è°ƒçš„åŒ»ç–—å¯¹è¯å¤§æ¨¡å‹ã€‚æˆ‘ä»¬ç»è¿‡è°ƒç ”å‘ç°ï¼Œåœ¨åŒ»ç–—é¢†åŸŸï¼Œå¾€å¾€åŒ»ç”Ÿéœ€è¦é€šè¿‡å¤šè½®é—®è¯¢æ‰èƒ½è¿›è¡Œå†³ç­–ï¼Œè¿™å¹¶ä¸æ˜¯å•çº¯çš„â€œæŒ‡ä»¤-å›å¤â€æ¨¡å¼ã€‚ç”¨æˆ·åœ¨å’¨è¯¢åŒ»ç”Ÿæ—¶ï¼Œå¾€å¾€ä¸ä¼šåœ¨æœ€åˆå°±æŠŠå®Œæ•´çš„æƒ…å†µå‘ŠçŸ¥åŒ»ç”Ÿï¼Œå› æ­¤åŒ»ç”Ÿéœ€è¦ä¸æ–­è¿›è¡Œè¯¢é—®ï¼Œæœ€åæ‰èƒ½è¿›è¡Œè¯Šæ–­å¹¶ç»™å‡ºåˆç†çš„å»ºè®®ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº† **æ‰é¹Š-1.0ï¼ˆBianQue-1.0ï¼‰** ï¼Œæ‹Ÿåœ¨ **å¼ºåŒ–AIç³»ç»Ÿçš„é—®è¯¢èƒ½åŠ›** ï¼Œä»è€Œè¾¾åˆ°æ¨¡æ‹ŸåŒ»ç”Ÿé—®è¯Šçš„è¿‡ç¨‹ã€‚æˆ‘ä»¬æŠŠè¿™ç§èƒ½åŠ›å®šä¹‰ä¸ºâ€œæœ›é—»é—®åˆ‡â€å½“ä¸­çš„â€œé—®â€ã€‚

ç»¼åˆè€ƒè™‘å½“å‰ä¸­æ–‡è¯­è¨€æ¨¡å‹æ¶æ„ã€å‚æ•°é‡ä»¥åŠæ‰€éœ€è¦çš„ç®—åŠ›ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†[ClueAI/ChatYuan-large-v2](https://huggingface.co/ClueAI/ChatYuan-large-v2)ä½œä¸ºåŸºå‡†æ¨¡å‹ï¼Œåœ¨8å¼  NVIDIA RTX 4090æ˜¾å¡ä¸Šå¾®è°ƒäº†1ä¸ªepochå¾—åˆ°**æ‰é¹Š-1.0ï¼ˆBianQue-1.0ï¼‰**ï¼Œç”¨äºè®­ç»ƒçš„**ä¸­æ–‡åŒ»ç–—é—®ç­”æŒ‡ä»¤ä¸å¤šè½®é—®è¯¢å¯¹è¯æ··åˆæ•°æ®é›†**åŒ…å«äº†è¶…è¿‡900ä¸‡æ¡æ ·æœ¬ï¼Œè¿™èŠ±è´¹äº†å¤§çº¦16å¤©çš„æ—¶é—´å®Œæˆä¸€ä¸ªepochçš„è®­ç»ƒã€‚

æˆ‘ä»¬å°†è®¡åˆ’å›´ç»•æ‰é¹Šæ¨¡å‹çš„â€œæœ›é—»é—®åˆ‡â€èƒ½åŠ›ï¼Œç»“åˆåŒ»å­¦ä¸“å®¶çŸ¥è¯†ã€å¤šæ¨¡æ€æŠ€æœ¯ã€å¤šç”Ÿç†ä¿¡å·è®¡ç®—ç­‰ï¼Œè¿›è¡Œå¤šä¸ªç‰ˆæœ¬çš„æ¨¡å‹è¿­ä»£ç ”ç©¶ã€‚

æ‰é¹Šï¼ˆBianQueï¼‰æ¨¡å‹æ¬¢è¿ä½ çš„è´¡çŒ®ï¼æˆ‘ä»¬é¼“åŠ±ä½ åœ¨ [BianQue GitHub](https://github.com/scutcyr/BianQue) é¡µé¢æŠ¥å‘Šé—®é¢˜ã€è´¡çŒ® PR å¹¶å‚ä¸è®¨è®ºã€‚æˆ‘ä»¬æœŸå¾…ä¸æ›´å¤šçš„é«˜æ ¡ã€åŒ»é™¢ã€ç ”ç©¶å®éªŒå®¤ã€å…¬å¸ç­‰è¿›è¡Œåˆä½œï¼Œå…±åŒå¼€å±•ä¸‹ä¸€ä»£æ‰é¹Šæ¨¡å‹ç ”ç©¶ã€‚å¯¹äºæ­¤ç±»éœ€æ±‚ï¼ˆä»¥åŠå…¶ä»–ä¸é€‚åˆåœ¨ GitHub ä¸Šæå‡ºçš„éœ€æ±‚ï¼‰ï¼Œè¯·ç›´æ¥å‘é€ç”µå­é‚®ä»¶è‡³ [eeyirongchen@mail.scut.edu.cn](mailto:eeyirongchen@mail.scut.edu.cn)ã€‚


## è®­ç»ƒæ•°æ®
æˆ‘ä»¬ç»“åˆå½“å‰å¼€æºçš„ä¸­æ–‡åŒ»ç–—é—®ç­”æ•°æ®é›†ï¼ˆ[MedDialog-CN](https://github.com/UCSD-AI4H/Medical-Dialogue-System)ã€[IMCS-V2](https://github.com/lemuria-wchen/imcs21)ã€[CHIP-MDCFNPC](https://tianchi.aliyun.com/dataset/95414)ã€[MedDG](https://tianchi.aliyun.com/dataset/95414)ã€[cMedQA2](https://github.com/zhangsheng93/cMedQA2)ã€[Chinese-medical-dialogue-data](https://github.com/Toyhom/Chinese-medical-dialogue-data)ï¼‰ï¼Œä»¥åŠè‡ªå»ºçš„æŒ‡ä»¤æ•°æ®é›†ï¼Œé€šè¿‡è¿›ä¸€æ­¥çš„æ•°æ®æ¸…æ´—ï¼Œæ„å»ºäº†ä¸€ä¸ªå¤§äº900ä¸‡æ¡æ ·æœ¬çš„**ä¸­æ–‡åŒ»ç–—é—®ç­”æŒ‡ä»¤ä¸å¤šè½®é—®è¯¢å¯¹è¯æ··åˆæ•°æ®é›†**ï¼Œæ•°æ®é›†çš„å¹³å‡è½®æ•°ä¸º3ï¼Œæœ€å¤§è½®æ•°è¾¾åˆ°218ï¼Œæ•°æ®æ ¼å¼ä¸ºï¼š
```data
input: "ç—…äººï¼šå…­å²å®å®æ‹‰å¤§ä¾¿éƒ½æ˜¯ä¸€ä¸ªç¤¼æ‹œæˆ–è€…10å¤©æ‰ä¸€æ¬¡æ­£å¸¸å—ï¼Œè¦å»åŒ»é™¢æ£€æŸ¥ä»€ä¹ˆé¡¹ç›®\nåŒ»ç”Ÿï¼šæ‚¨å¥½\nç—…äººï¼šå…­å²å®å®æ‹‰å¤§ä¾¿éƒ½æ˜¯ä¸€ä¸ªç¤¼æ‹œæˆ–è€…10å¤©æ‰ä¸€æ¬¡æ­£å¸¸å—ï¼Œè¦å»åŒ»é™¢æ£€æŸ¥ä»€ä¹ˆé¡¹ç›®\nåŒ»ç”Ÿï¼šå®å®ä¹‹å‰å¤§ä¾¿ä»€ä¹ˆæ ·å‘¢ï¼Ÿå¤šä¹…ä¸€æ¬¡å‘¢\nç—…äººï¼šä¸€èˆ¬éƒ½æ˜¯ä¸€ä¸ªç¤¼æ‹œï¼Œæœ€è¿‘è¿™å‡ ä¸ªæœˆéƒ½æ˜¯10å¤šå¤©\nåŒ»ç”Ÿï¼šå¤§ä¾¿å¹²å—ï¼Ÿ\nç—…äººï¼šæ¯æ¬¡10å¤šå¤©æ‹‰çš„å¾ˆå¤š\nåŒ»ç”Ÿï¼š"
target: "æˆå½¢è¿˜æ˜¯ä¸æˆå½¢å‘¢ï¼Ÿå­©å­åƒé¥­æ€ä¹ˆæ ·å‘¢ï¼Ÿ"
```
è®­ç»ƒæ•°æ®å½“ä¸­æ··åˆäº†å¤§é‡targetæ–‡æœ¬ä¸º**åŒ»ç”Ÿé—®è¯¢çš„å†…å®¹**è€Œéç›´æ¥çš„å»ºè®®ï¼Œè¿™å°†æœ‰åŠ©äºæå‡AIæ¨¡å‹çš„é—®è¯¢èƒ½åŠ›ã€‚


## æ¨¡å‹â€œé—®â€èƒ½åŠ›ç¤ºä¾‹
â€œæœ›é—»é—®åˆ‡â€å››è¯Šæ³•ç”±æ‰é¹Šå‘æ˜ã€‚â€œå››è¯Šæ³•â€ï¼ˆæœ›ã€é—»ã€é—®ã€åˆ‡ï¼‰ï¼Œæ˜¯ä¸­åŒ»è¯Šç—…çš„åŸºæœ¬æ–¹æ³•ã€‚å…¶ä¸­ï¼Œâ€œé—®â€çš„èƒ½åŠ›å¯ä»¥è¢«å»ºæ¨¡åˆ°è¯­è¨€æ¨¡å‹å½“ä¸­ã€‚æˆ‘ä»¬æŠŠAIæ¨¡å‹çš„â€œé—®â€èƒ½åŠ›å®šä¹‰ä¸ºï¼š    
**â€œé—®â€èƒ½åŠ›**ï¼šAIæ¨¡å‹èƒ½å¤Ÿæ ¹æ®ç—…äººè¾“å…¥çš„ä¿¡æ¯ï¼Œç»“åˆåŒ»å­¦çŸ¥è¯†æ¥è¿›è¡Œ**æ°å½“çš„é—®è¯¢**ï¼Œå¹¶ä¸”ç»“åˆè¿›ä¸€æ­¥çš„é—®è¯¢å¾—åˆ°çš„ç—…äººçš„ä¿¡æ¯ï¼Œç»™å‡ºâ€œåˆé€‚çš„è¯Šæ–­æˆ–å»ºè®®â€ã€‚â€œé—®â€çš„è¿‡ç¨‹é€šå¸¸æ˜¯ä¸€ä¸ªå¤šè½®å¯¹è¯è¿‡ç¨‹ã€‚

ä»¥ä¸‹æ˜¯2ä¸ªç®€å•çš„ä¾‹å­ï¼š

* **ä¾‹å­1**    
ç”¨æˆ·ï¼šæˆ‘æœ€è¿‘æ„Ÿè§‰å…¨èº«ç–²æƒ«   
æ‰é¹Š-1.0ï¼šæ˜¯åŠ³ç´¯äº†ï¼Œè¿˜æ˜¯ç†¬å¤œäº†ï¼Ÿ    
ç”¨æˆ·ï¼šè¿™å‘¨éƒ½åœ¨ç†¬å¤œèµ¶è®ºæ–‡    
æ‰é¹Š-1.0ï¼šå—¯ï¼Œç†¬å¤œå®¹æ˜“å¼•èµ·å„ç§ä¸é€‚ï¼Œå¦‚å¤´æ™•ã€ä¹åŠ›ã€å¤±çœ ã€å¤šæ¢¦ç­‰ï¼Œå¤šä¼‘æ¯ï¼Œå¯ä»¥å£æœè°·ç»´ç´ ã€ç»´ç”Ÿç´ B1å’Œå®‰ç¥è¡¥è„‘æ¶²ï¼Œé¿å…ç†¬å¤œã€‚    

* **ä¾‹å­2**   
ç”¨æˆ·ï¼šæˆ‘æœ€è¿‘ç»å¸¸å¤±çœ     
æ‰é¹Š-1.0ï¼šå¤±çœ å¤šä¹…äº†ï¼Ÿ    
ç”¨æˆ·ï¼šä¸¤å‘¨äº†    
æ‰é¹Š-1.0ï¼šç¡çœ æ€ä¹ˆæ ·ï¼Ÿ    
ç”¨æˆ·ï¼šä¸ŠåºŠå‡ å°æ—¶æ‰ç¡å¾—ç€    
æ‰é¹Š-1.0ï¼šå¯ä»¥å£æœä¸€äº›è¯ç‰©   
ç”¨æˆ·ï¼šæœ‰å…·ä½“æ¨èå—   
æ‰é¹Š-1.0ï¼šéœ€è¦å…ˆæ£€æŸ¥ä¸€ä¸‹   
ç”¨æˆ·ï¼šé‚£æˆ‘éœ€è¦åšä»€ä¹ˆæ£€æŸ¥ï¼Ÿ   
æ‰é¹Š-1.0ï¼šå¿ƒç”µå›¾   


## ä½¿ç”¨æ–¹æ³•

### ç›´æ¥ä½¿ç”¨æ‰é¹Š-1.0æ¨¡å‹

```python
import os
from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoTokenizer
import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


tokenizer = T5Tokenizer.from_pretrained("scutcyr/BianQue-1.0")
model = T5ForConditionalGeneration.from_pretrained("scutcyr/BianQue-1.0")


def preprocess(text):
    text = text.replace("\n", "\\n").replace("\t", "\\t")
    return text

def postprocess(text):
    return text.replace("\\n", "\n").replace("\\t", "\t")

def answer(user_history, bot_history, sample=True, top_p=1, temperature=0.7):
    '''sampleï¼šæ˜¯å¦æŠ½æ ·ã€‚ç”Ÿæˆä»»åŠ¡ï¼Œå¯ä»¥è®¾ç½®ä¸ºTrue;
    top_pï¼š0-1ä¹‹é—´ï¼Œç”Ÿæˆçš„å†…å®¹è¶Šå¤šæ ·
    max_new_tokens=512 lost...'''

    if len(bot_history)>0:
        context = "\n".join([f"ç—…äººï¼š{user_history[i]}\nåŒ»ç”Ÿï¼š{bot_history[i]}" for i in range(len(bot_history))])
        input_text = context + "\nç—…äººï¼š" + user_history[-1] + "\nåŒ»ç”Ÿï¼š"
    else:
        input_text = "ç—…äººï¼š" + user_history[-1] + "\nåŒ»ç”Ÿï¼š"
        return "æˆ‘æ˜¯åˆ©ç”¨äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œç»“åˆå¤§æ•°æ®è®­ç»ƒå¾—åˆ°çš„æ™ºèƒ½åŒ»ç–—é—®ç­”æ¨¡å‹æ‰é¹Šï¼Œä½ å¯ä»¥å‘æˆ‘æé—®ã€‚"
    

    input_text = preprocess(input_text)
    print(input_text)
    encoding = tokenizer(text=input_text, truncation=True, padding=True, max_length=768, return_tensors="pt").to(device) 
    if not sample:
        out = model.generate(**encoding, return_dict_in_generate=True, output_scores=False, max_new_tokens=512, num_beams=1, length_penalty=0.6)
    else:
        out = model.generate(**encoding, return_dict_in_generate=True, output_scores=False, max_new_tokens=512, do_sample=True, top_p=top_p, temperature=temperature, no_repeat_ngram_size=3)
    out_text = tokenizer.batch_decode(out["sequences"], skip_special_tokens=True)
    print('åŒ»ç”Ÿ: '+postprocess(out_text[0]))
    return postprocess(out_text[0])

answer_text = answer(user_history=["ä½ å¥½ï¼",
                                   "æˆ‘æœ€è¿‘ç»å¸¸å¤±çœ ",
                                   "ä¸¤å‘¨äº†",
                                   "ä¸ŠåºŠå‡ å°æ—¶æ‰ç¡å¾—ç€"], 
                     bot_history=["æˆ‘æ˜¯åˆ©ç”¨äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œç»“åˆå¤§æ•°æ®è®­ç»ƒå¾—åˆ°çš„æ™ºèƒ½åŒ»ç–—é—®ç­”æ¨¡å‹æ‰é¹Šï¼Œä½ å¯ä»¥å‘æˆ‘æé—®ã€‚",
                                  "å¤±çœ å¤šä¹…äº†ï¼Ÿ",
                                  "ç¡çœ æ€ä¹ˆæ ·ï¼Ÿ"])
```

### ç›´ä½¿ç”¨ä¸ªäººæ•°æ®åœ¨æ‰é¹Š-1.0æ¨¡å‹åŸºç¡€ä¸Šè¿›ä¸€æ­¥å¾®è°ƒæ¨¡å‹
* ç¯å¢ƒåˆ›å»º   
ä»¥ä¸‹ä¸ºåœ¨RTX 4090æ˜¾å¡ï¼ŒCUDA-11.6é©±åŠ¨é…ç½®ä¸‹çš„ç¯å¢ƒé…ç½®
```bash
conda env create -n bianque_py38 --file py38_conda_env.yml
conda activate bianque_py38
pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116
```
* æ•°æ®é›†æ„å»º
å‚è€ƒ[.data/cMedialog_example.csv](.data/cMedialog_example.csv)æ ¼å¼ï¼Œæ„å»ºä½ çš„æ•°æ®é›†
* åŸºäºæ‰é¹Š-1.0æ¨¡å‹å¾®è°ƒä½ çš„æ¨¡å‹
ä¿®æ”¹[./scripts/run_train_model_bianque.sh]ï¼Œé€šè¿‡ç»å¯¹è·¯å¾„æŒ‡å®šPREPROCESS_DATAï¼Œå¹¶ä¸”è°ƒæ•´å…¶ä»–å˜é‡ï¼Œç„¶åè¿è¡Œï¼š
```bash
cd scripts
bash run_train_model_bianque.sh
```


## å£°æ˜

**æ‰é¹Š-1.0ï¼ˆBianQue-1.0ï¼‰**å½“å‰ä»…ç»è¿‡1ä¸ªepochçš„è®­ç»ƒï¼Œå°½ç®¡æ¨¡å‹å…·å¤‡äº†ä¸€å®šçš„åŒ»ç–—é—®è¯¢èƒ½åŠ›ï¼Œä½†å…¶ä»ç„¶å­˜åœ¨ä»¥ä¸‹å±€é™ï¼š
* è®­ç»ƒæ•°æ®æ¥æºäºå¼€æºæ•°æ®é›†ä»¥åŠäº’è”ç½‘ï¼Œå°½ç®¡æˆ‘ä»¬é‡‡ç”¨äº†ä¸¥æ ¼çš„æ•°æ®æ¸…æ´—æµç¨‹ï¼Œæ•°æ®é›†å½“ä¸­ä»ç„¶ä¸å¯é¿å…åœ°å­˜åœ¨å¤§é‡å™ªå£°ï¼Œè¿™ä¼šä½¿å¾—éƒ¨åˆ†å›å¤äº§ç”Ÿé”™è¯¯ï¼›
* åŒ»ç”Ÿâ€œé—®è¯¢â€æ˜¯ä¸€é¡¹å¤æ‚çš„èƒ½åŠ›ï¼Œè¿™æ˜¯éåŒ»ç”Ÿç¾¤ä½“æ‰€ä¸å…·å¤‡çš„ï¼Œå½“å‰çš„æ¨¡å‹å¯¹äºæ¨¡æ‹Ÿâ€œåŒ»ç”Ÿé—®è¯¢â€è¿‡ç¨‹æ˜¯é€šè¿‡å¤§é‡æ ·æœ¬å­¦ä¹ å¾—åˆ°çš„ï¼Œå› æ­¤åœ¨é—®è¯¢è¿‡ç¨‹å½“ä¸­ï¼Œæœ‰å¯èƒ½å‡ºç°ä¸€äº›å¥‡å¼‚çš„æé—®é£æ ¼ã€‚æ¢ä¸€å¥è¯æ¥è¯´ï¼Œå½“å‰ç‰ˆæœ¬çš„æ¨¡å‹å¼ºåŒ–äº†â€œé—®â€çš„èƒ½åŠ›ï¼Œä½†æ˜¯â€œæœ›â€ã€â€œé—»â€ã€â€œåˆ‡â€çš„èƒ½åŠ›ä»å¾…è¿›ä¸€æ­¥ç ”ç©¶ï¼


## å¼•ç”¨
```bib
@article{chen2023bianque1,
      title={BianQue-1.0: Improving the "Question" Ability of Medical Chat Model through finetuning with Hybrid Instructions and Multi-turn Doctor QA Datasets}, 
      author={Yirong Chen and Zhenyu Wang and Xiaofen Xing and Zhipei Xu and Kai Fang and Sihang Li and Junhong Wang and Xiangmin Xu},
      year={2023},
      url={https://github.com/scutcyr/BianQue}
}
```
